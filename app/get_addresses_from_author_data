# %%
import asyncio
import nest_asyncio
import aiohttp
import logging
import json
from tqdm.asyncio import tqdm
import pandas as pd
from tqdm import tqdm
from get_address import search_place, filter_best_match

# %%
with open('../config.json', 'r') as file:
    config = json.load(file)
google_maps_places_api_key = config['apiKeys']['googleMapsPlaces']

# %%
result_df = pd.read_pickle('./outputs/addresses_from_names/rneasy_authors.pkl')
print("RESULT DF:\n", result_df)

# %%
rneasy_df = pd.read_csv('/Users/nicoletrieu/Documents/zymo/metadata-scraper/app/data/rneasy_20_23.csv')
print("RNEASY DF:\n", rneasy_df)


# %%
def get_intials(row):
    initials = row['FirstName'][0]
    if pd.notnull(row['MiddleName']):
        middle_names = row['MiddleName'].split()
        for name in middle_names:
            initials += name[0]

    return initials


# %%
rneasy_df['Initials'] = rneasy_df.apply(get_intials, axis=1)
print("RNEASY DF WITH INITIALS:\n", rneasy_df)

# %%
merged_df = pd.merge(result_df, rneasy_df, left_on=['initials', 'lastName'], right_on=['Initials', 'LastName'], how='inner')
filtered_df = merged_df[['initials', 'lastName', 'affiliation', 'institute']]
print("FILTERED DF:\n", filtered_df)

# %%
deduped_filtered_df = filtered_df.drop_duplicates()
print("DEDUPED FILTERED DF:\n", deduped_filtered_df)

# %%
author_dicts = deduped_filtered_df.to_dict('records')

# %%
print("length of author_dicts:", len(author_dicts))

# %%
print("first few author dicts:", author_dicts[:5])


# %%
def get_address_from_author_dicts(author_dicts: list, api_key: str):
    all_results = []

    for author_dict in tqdm(author_dicts, desc="Getting Addresses"):
        for key in ['affiliation', 'institute']:
            if author_dict.get(key, "") == "Unparsed":  # Skip 'Unparsed' values
                continue

            search_result = search_place(author_dict[key], api_key)

            if search_result == {}:
                continue  # Skip empty search result
            if search_result.get("error"):
                print(search_result.get("message"))  # Log the error message
                continue

            result_list = search_result.get("places", [])
            if not result_list:
                continue  # Skip if no results found

            # Find the best match:
            best_match_address = filter_best_match(result_list, author_dict[key])
            if best_match_address:
                result_dict = {
                    "intials": author_dict.get('initials'),
                    "lastName": author_dict.get('lastName'),
                    "affiliation": author_dict.get('affiliation', "Unspecified"),
                    "institute": author_dict.get('institute', "Unparsed"),
                    "address": best_match_address
                }
                all_results.append(result_dict)
                break  # Assuming you only want one address per author

    return all_results


# %%
address_dicts = get_address_from_author_dicts(author_dicts, google_maps_places_api_key)

# %%
address_df = pd.DataFrame(address_dicts)
address_df.to_pickle('./outputs/addresses_from_names/rneasy_addresses.pkl')

# %%
address_df = pd.read_pickle('./outputs/addresses_from_names/rneasy_addresses.pkl')
print("ADDRESS DF:\n", address_df)

# %%
print("RNEASY DF:\n", rneasy_df)

# %%
filename = './outputs/addresses_from_names/rneasy_addresses.csv'
address_df.to_csv(filename, index=False)

# %%
rneasy_renamed_df = rneasy_df.rename(columns={'LastName': 'lastName', 'Organisation': 'institute'})
print("RNEASY RENAMED DF:\n", rneasy_renamed_df)

# %% Get rows with matching last name and institute
matching_df = pd.merge(address_df, rneasy_renamed_df, on=['lastName', 'institute']).drop_duplicates().reset_index(drop=True)
print("MATCHING ROWS DF:\n", matching_df)

# %%
filename = './outputs/addresses_from_names/matched_rneasy_addresses.csv'
matching_df.to_csv(filename, index=False)

# %%
deduped_filtered_df['unique_id'] = deduped_filtered_df['lastName'].str.lower() + deduped_filtered_df['initials'].str.lower() + deduped_filtered_df['institute'].str.lower()
matching_df['unique_id'] = matching_df['FirstName'].str.lower() + matching_df['lastName'].str.lower() + matching_df['institute'].str.lower()
remaining_leads_df = deduped_filtered_df[~deduped_filtered_df['unique_id'].isin(matching_df['unique_id'])]
print("FILTERED RNEASY DF:\n", remaining_leads_df)

# %%
remaining_author_dicts = remaining_leads_df.apply(lambda row: {
    "initials": row["Initials"],
    "lastName": row["lastName"],
    "affiliation": row["affiliation"],
    "institute": row["institute"]
}, axis=1).tolist()

print("first few remaining author dicts:", remaining_author_dicts[:5])

# # %%
# unparsed_institute_df = address_df[address_df['institute'] == 'Unparsed']
# print("UNPARSED INSTITUTE DF:\n", unparsed_institute_df)

# # %%
# filtered_address_df = pd.concat([matching_df, unparsed_institute_df]).drop_duplicates().reset_index(drop=True)
# print("FILTERED ADDRESS DF:\n", filtered_address_df)

# # %%
# filename = './outputs/addresses_from_names/filtered_rneasy_addresses.csv'
# filtered_address_df.to_csv(filename, index=False)


# %%
async def search_place_async(place, api_key):
    url = 'https://places.googleapis.com/v1/places:searchText'
    payload = {"textQuery": f"address for {place}"}
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': api_key,
        'X-Goog-FieldMask': 'places.displayName,places.formattedAddress,places.priceLevel'
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    return {"error": True, "status_code": response.status, "message": await response.text}

        except aiohttp.ClientError as e:
            logging.error(f'HTTP client error occurred: {e}')
            return {"error": True, "message": f'HTTP client error occurred: {e}'}
        except asyncio.TimeoutError as e:
            logging.error(f'Timeout error occurred: {e}')
            return {"error": True, "message": f'Timeout error occurred: {e}'}
        except Exception as e:
            logging.error(f'An unexpected error occurred: {e}')
            return {"error": True, "message": f'An unexpected error occurred: {e}'}


# %%
async def get_address_from_author_dicts_async(author_dicts: list, api_key: str):
    all_results = []

    async def process_author_dict(author_dict):
        for key in ['affiliation', 'institute']:
            if author_dict.get(key, "") == "Unparsed":
                continue

            search_result = await search_place_async(author_dict[key], api_key)

            if search_result == {}:
                continue
            if search_result.get("error"):
                print(search_result.get("message"))
                continue

            result_list = search_result.get("places", [])
            if not result_list:
                continue

            best_match_address = filter_best_match(result_list, author_dict[key])  # Assuming this is a synchronous function
            if best_match_address:
                return {
                    "intials": author_dict.get('initials'),
                    "lastName": author_dict.get('lastName'),
                    "affiliation": author_dict.get('affiliation', "Unspecified"),
                    "institute": author_dict.get('institute', "Unparsed"),
                    "address": best_match_address
                }

    tasks = [process_author_dict(author_dict) for author_dict in author_dicts]
    for i in tqdm(range(0, len(tasks), 10), desc="Getting Addresses"):
        batch = tasks[i:i+10]
        results = await asyncio.gather(*batch)
        # Filter out None results and extend all_results
        all_results.extend([result for result in results if result])
        await asyncio.sleep(1)  # Sleep to respect the rate limit of 10 req/s

    return all_results


# %%
nest_asyncio.apply()

loop = asyncio.get_event_loop()
if loop.is_running():
    task = asyncio.ensure_future(get_address_from_author_dicts_async(
            author_dicts, google_maps_places_api_key
        ))
    address_dicts = loop.run_until_complete(task)
else:
    address_dicts = loop.run_until_complete(
        get_address_from_author_dicts_async(
            author_dicts, google_maps_places_api_key
        )
    )
